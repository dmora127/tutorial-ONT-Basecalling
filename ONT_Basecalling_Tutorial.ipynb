{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07326ae",
   "metadata": {},
   "source": [
    "\n",
    "# Long‑Read Basecalling on the OSPool (ONT → Dorado, GPU)\n",
    "\n",
    "**Goal:** Basecall Oxford Nanopore **POD5** files into **BAM/FASTQ** using **Dorado (GPU)** on the **OSPool**, organized as a scalable, HTCondor‑driven workflow.  \n",
    "**Data locality:** Input and outputs are staged via the **Open Science Data Federation (OSDF)**.  \n",
    "**Compute:** GPU‑enabled OSPool execution points.\n",
    "\n",
    "> This notebook mirrors the structure and style of the Minimap2 tutorial you provided, but focuses **only** on the *basecalling* stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4dc77d",
   "metadata": {},
   "source": [
    "\n",
    "## Learning outcomes\n",
    "\n",
    "By the end, you can:\n",
    "- Organize ONT inputs (POD5) and model archives for **Dorado** on the OSPool.\n",
    "- Create an **HTCondor** submit file for GPU basecalling at scale.\n",
    "- Launch, monitor, and debug jobs.\n",
    "- Collect basecalled outputs from OSDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f09e87",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "- OSPool account/access (OSG/CHTC).\n",
    "- Basic command‑line skills (Bash, file paths).\n",
    "- HTCondor basics (submit files, logs).\n",
    "- Your ONT raw data in **POD5** format and a Dorado model tarball (e.g., `dna_r10.4.1_e8.2_400bps_hac@v5.2.0.tar.gz`).\n",
    "- A working Dorado container or the tutorial container environment.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understanding Basecalling in Oxford Nanopore Sequencing\n",
    "\n",
    "In Oxford Nanopore Technologies (ONT) sequencing, the instrument does not directly read DNA or RNA bases. Instead, it measures subtle changes in ionic current as a single-stranded molecule passes through a nanopore embedded in a membrane. These electrical current traces—often called raw signals or squiggles—reflect the sequence-dependent resistance patterns of the nucleotides inside the pore.\n",
    "\n",
    "### From Signal to Sequence: The Role of Basecalling\n",
    "\n",
    "The process of converting these continuous electrical signals into nucleotide sequences (A, C, G, T or U) is called basecalling. Basecalling algorithms interpret the temporal and amplitude patterns in the current signal to infer the most probable underlying sequence of bases. This is one of the most computationally demanding and algorithmically sophisticated parts of the ONT analysis pipeline.\n",
    "\n",
    "Dorado uses deep neural network models trained to map signal patterns to the most probable base sequence. These models are computationally intensive, relying on GPU acceleration to perform millions of operations per second. GPUs dramatically shorten inference time by parallelizing the math that drives neural network prediction, enabling accurate and high-throughput decoding of long-read data.\n",
    "\n",
    "Because each read or POD5 file can be basecalled independently, Dorado workflows scale perfectly on the OSPool, where thousands of GPU-enabled jobs can run simultaneously. The OSPool’s distributed architecture provides the compute, memory, and data-staging infrastructure (via OSDF) needed to handle large sequencing runs reproducibly and efficiently, turning hours of local computation into minutes of parallel basecalling across the national HTC fabric.\n",
    "\n",
    "## Basecalling on the OSPool by Sequencing Channel\n",
    "\n",
    "When performing simplex basecalling with Dorado, it’s often advantageous to reorganize your raw data by sequencing channel before submitting jobs to the OSPool. Each Oxford Nanopore flow cell consists of hundreds to thousands of channels that generate independent signal traces, meaning the data within a single POD5 file can be subdivided into smaller, channel-specific subsets.\n",
    "\n",
    "By splitting the data so that each channel’s reads reside in their own POD5 file, we enable truly parallel basecalling: each channel file becomes an independent job that can run simultaneously across hundreds or thousands of OSPool execution points. This design perfectly aligns with the principles of High Throughput Computing (HTC)—many small, independent jobs working together to accelerate large workflows.\n",
    "\n",
    "We use the POD5 package available inside the dorado.sif container to generate per-channel subsets and organize them into a split_pod5_subsets directory. Once subdivided, each of these new POD5 files can be basecalled individually, dramatically reducing time-to-results while maintaining full reproducibility and scalability on the OSPool."
   ],
   "id": "25dd3ca4f02fbe71"
  },
  {
   "cell_type": "markdown",
   "id": "67535915",
   "metadata": {},
   "source": [
    "\n",
    "## Directory structure (recommended)\n",
    "\n",
    "We assume a layout similar to this (directories may already exist if you used the companion repository):\n",
    "\n",
    "```\n",
    "ont-basecalling/\n",
    "├── executables/               # helper scripts\n",
    "├── inputs/                    # POD5 inputs (or OSDF paths to them)\n",
    "├── logs/                      # condor .log/.out/.err\n",
    "├── outputs/                   # basecalled outputs (BAM/FASTQ)\n",
    "├── software/                  # containers, model tarballs, helper assets\n",
    "├── list_of_pod5_files.txt     # one POD5 path per line\n",
    "├── run_dorado.sub             # HTCondor submit file (GPU)\n",
    "└── tutorial-setup.sh          # optional helper to set up the tree\n",
    "```\n",
    "\n",
    "You will also have a companion **OSDF** directory for storing large files, such as containers and Dorado models. The directory structure should look like:\n",
    "\n",
    "```\n",
    "/ospool/<ap##>/data/<your-username>/tutorial-ONT-Basecalling/\n",
    "\n",
    "├── data                       # Dorado model tarballs\n",
    "├── ├── dna_r10.4.1_e8.2_400bps_fast@v4.2.0_5mCG_5hmCG@v2.tar.gz\n",
    "├── ├── dna_r10.4.1_e8.2_400bps_fast@v4.2.0.tar.gz\n",
    "├── ├── ...\n",
    "├── ├── rna004_130bps_sup@v5.2.0.tar.gz\n",
    "├── software/                  # Dorado container (or osdf:/// path)\n",
    "└── ├── dorado_build1.2.0_27OCT2025_v1.sif\n",
    "```\n",
    "\n",
    "We've included a `tutorial-setup.sh` script in the companion repository to create this structure for you. To run it, execute:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "./tutorial-setup.sh ",
   "id": "1369677923ad70f2"
  },
  {
   "cell_type": "markdown",
   "id": "dcace00f",
   "metadata": {},
   "source": [
    "\n",
    "## OSDF data paths\n",
    "\n",
    "For large datasets, place/read inputs via **OSDF** and write outputs back to OSDF. Example patterns:\n",
    "\n",
    "- OSDF:\n",
    "  `/ospool/<ap##>/data/<your-username>/tutorial-ONT-Basecalling/`\n",
    "\n",
    "You can also stage a **models** tarball on OSDF and fetch it at job start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682dbaf5",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare the list of POD5 files\n",
    "\n",
    "List one absolute OSDF path per line in `list_of_pod5_files.txt`. This enables scalable, itemized submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a starter list file (edit paths to your inputs).\n",
    "# You can paste more lines or generate this programmatically.\n",
    "%%bash\n",
    "cat > list_of_pod5_files.txt << 'EOF'\n",
    "/ospool/uc-shared/public/example-ont/pod5/sample01/channel-100.pod5\n",
    "/ospool/uc-shared/public/example-ont/pod5/sample01/channel-101.pod5\n",
    "# Add more POD5 files, one per line...\n",
    "EOF\n",
    "\n",
    "echo \"Wrote $(wc -l < list_of_pod5_files.txt) entries to list_of_pod5_files.txt\"\n",
    "head -n 3 list_of_pod5_files.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0f752",
   "metadata": {},
   "source": [
    "\n",
    "## Create an executable wrapper for Dorado\n",
    "\n",
    "This wrapper untars the Dorado model **per job**, runs Dorado basecalling on one POD5 file, and emits a BAM (and optionally FASTQ).  \n",
    "Adapt the `DORADO_ARGS` and `DORADO_MODEL_TARBALL` names to your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p executables outputs logs software\n",
    "\n",
    "cat > executables/run_dorado.sh << 'EOS'\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Positional args\n",
    "DORADO_ARGS=\"$1\"           # e.g., 'basecaller --device cuda:all --batchsize 16 hac@v5.0.0'\n",
    "DORADO_MODEL_TARBALL=\"$2\"  # e.g., 'dna_r10.4.1_e8.2_400bps_hac@v5.2.0.tar.gz'\n",
    "INPUT_POD5=\"$3\"            # one .pod5 file (from list_of_pod5_files.txt)\n",
    "\n",
    "# Derive filenames\n",
    "BAM_FILE=\"${INPUT_POD5##*/}.bam\"   # channel-100.pod5.bam\n",
    "FASTQ_FILE=\"${BAM_FILE}.fastq\"     # optional\n",
    "\n",
    "echo \"[INFO] Host: $(hostname)\"\n",
    "echo \"[INFO] GPU(s): ${CUDA_VISIBLE_DEVICES:-'none'}\"\n",
    "echo \"[INFO] Input POD5: ${INPUT_POD5}\"\n",
    "echo \"[INFO] Dorado args: ${DORADO_ARGS}\"\n",
    "echo \"[INFO] Model tarball: ${DORADO_MODEL_TARBALL}\"\n",
    "\n",
    "# Model extraction (assumes tarball in the job sandbox)\n",
    "tar -xzf \"${DORADO_MODEL_TARBALL}\"\n",
    "rm -f \"${DORADO_MODEL_TARBALL}\"\n",
    "echo \"[INFO] Model extracted.\"\n",
    "\n",
    "# Run Dorado (produces BAM on stdout)\n",
    "# Example DORADO_ARGS:\n",
    "#   \"basecaller --device cuda:all --batchsize 16 hac@v5.0.0\"\n",
    "# Replace with your specific model alias, e.g. hac@v5.2.0\n",
    "set -x\n",
    "dorado ${DORADO_ARGS} \"${INPUT_POD5}\" > \"${BAM_FILE}\"\n",
    "set +x\n",
    "echo \"[INFO] Dorado basecalling complete: ${BAM_FILE}\"\n",
    "\n",
    "# Optional: convert BAM -> FASTQ\n",
    "# samtools fastq \"${BAM_FILE}\" > \"${FASTQ_FILE}\"\n",
    "# echo \"[INFO] Generated FASTQ: ${FASTQ_FILE}\"\n",
    "\n",
    "# Stage outputs (this directory is transferred back or written to OSDF mount by HTCondor setup)\n",
    "mkdir -p outputs\n",
    "mv -v \"${BAM_FILE}\" outputs/\n",
    "# mv -v \"${FASTQ_FILE}\" outputs/  # if enabled\n",
    "EOS\n",
    "\n",
    "chmod +x executables/run_dorado.sh\n",
    "echo \"Created executables/run_dorado.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c93f0",
   "metadata": {},
   "source": [
    "\n",
    "## HTCondor submit file (GPU)\n",
    "\n",
    "This submit file requests a GPU slot, transfers in the model tarball and executable, and runs one POD5 per job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf08bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > run_dorado.sub << 'EOS'\n",
    "# Dorado basecalling on OSPool (GPU)\n",
    "universe                = vanilla\n",
    "executable              = executables/run_dorado.sh\n",
    "\n",
    "# Arguments: \"<DORADO_ARGS>\" \"<MODEL_TARBALL>\" \"<INPUT_POD5>\"\n",
    "# Example args use all visible GPUs; adapt to your device setup and model alias.\n",
    "arguments               = \"basecaller --device cuda:all --batchsize 16 hac@v5.0.0\" \"dna_r10.4.1_e8.2_400bps_hac@v5.2.0.tar.gz\" \"$(ITEM)\"\n",
    "\n",
    "# Input list (one pod5 per line)\n",
    "queue ITEM from list_of_pod5_files.txt\n",
    "\n",
    "# Files to transfer in with each job (executable is sent automatically)\n",
    "transfer_input_files    = software/dorado.sif, software/dna_r10.4.1_e8.2_400bps_hac@v5.2.0.tar.gz\n",
    "\n",
    "# Use Apptainer/Singularity container for Dorado\n",
    "# Adjust to your OSDF image path if preferred (e.g., osdf:///ospool/.../dorado.sif).\n",
    "+SingularityImage       = \"software/dorado.sif\"\n",
    "\n",
    "# GPU request and resource hints\n",
    "request_gpus            = 1\n",
    "request_cpus            = 2\n",
    "request_memory          = 8 GB\n",
    "request_disk            = 5 GB\n",
    "\n",
    "# Nice-to-have scheduling hints (adapt to your pool attributes)\n",
    "requirements            = (CUDACapability >= 7.0) && (HasDocker == false)\n",
    "\n",
    "# Logs\n",
    "log                     = logs/$(Cluster).log\n",
    "output                  = logs/$(Cluster).$(Process).out\n",
    "error                   = logs/$(Cluster).$(Process).err\n",
    "\n",
    "# File transfer behavior\n",
    "should_transfer_files   = YES\n",
    "when_to_transfer_output = ON_EXIT\n",
    "\n",
    "# Keep job sandbox small\n",
    "# periodic_remove = (JobStatus == 5) && (time() - QDate > 86400)\n",
    "EOS\n",
    "\n",
    "echo \"Created run_dorado.sub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c644b",
   "metadata": {},
   "source": [
    "\n",
    "### Place container and model on OSDF (recommended)\n",
    "\n",
    "For large classes, store the **Dorado container** and **model tarball** on OSDF and reference them via `osdf:///` URLs in `transfer_input_files`.  \n",
    "In this notebook we referenced local `software/dorado.sif` and a local model tarball for simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeeeff1",
   "metadata": {},
   "source": [
    "\n",
    "## Submit jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# Dry-run: print the first few expanded queue items for sanity\n",
    "echo \"Preview first 2 jobs:\"\n",
    "head -n 2 list_of_pod5_files.txt | while read -r p; do\n",
    "  echo condor_submit run_dorado.sub \"with ITEM=$p\"\n",
    "done\n",
    "\n",
    "# Actual submission (uncomment to run on your site)\n",
    "# condor_submit run_dorado.sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93dc10",
   "metadata": {},
   "source": [
    "\n",
    "## Monitor jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195250a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# Common monitoring commands\n",
    "echo \"Queue summary:\"\n",
    "condor_q -autoformat ClusterId ProcId JobStatus RequestGPUs | head -n 20 || true\n",
    "\n",
    "echo\n",
    "echo \"GPU slots overview (sample):\"\n",
    "condor_status -compact -constraint 'CUDACapability>=7.0' -af Name CUDACapability GPUs_Total GPUs_Allocated || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf230132",
   "metadata": {},
   "source": [
    "\n",
    "## Inspect logs and failures\n",
    "\n",
    "Look at `.out/.err` for Dorado error messages (e.g., model alias mismatch, no visible GPUs, CUDA driver mismatch).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f11e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# Tail a recent job's output/error (replace IDs accordingly)\n",
    "# tail -n 100 logs/<CLUSTER>.<PROC>.out\n",
    "# tail -n 100 logs/<CLUSTER>.<PROC>.err\n",
    "echo \"Replace <CLUSTER> and <PROC> with your IDs to inspect logs.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ed402",
   "metadata": {},
   "source": [
    "\n",
    "## Collect outputs\n",
    "\n",
    "Successful jobs write BAMs into `outputs/`. You can stage these back to OSDF or keep locally for downstream steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "mkdir -p outputs\n",
    "ls -lh outputs || true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05936414",
   "metadata": {},
   "source": [
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **`Invalid CUDA device index` / `0 visible CUDA devices`**: Ensure GPU slots are selected and container sees GPUs. Check `CUDACapability` and `nvidia-smi` availability.\n",
    "- **Model alias mismatch**: Confirm the `hac@...` / `sup@...` alias matches the model files within your tarball.\n",
    "- **Large model tarball transfer**: Host the model on **OSDF** to avoid per‑job upload from submit host.\n",
    "- **Throughput**: Tune `--batchsize`, `request_cpus`, and ensure enough GPU memory. Use job splitting on the POD5 granularity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e811f0",
   "metadata": {},
   "source": [
    "\n",
    "## Cleanup (optional)\n",
    "\n",
    "Remove temporary logs or partial outputs if you are iterating frequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21cc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "# rm -rf logs/* outputs/*  # uncomment with caution\n",
    "echo \"Nothing deleted. Uncomment the rm lines to clean.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf753fdd",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "- Proceed to **read mapping** (Minimap2) using your basecalled BAM/FASTQ files.\n",
    "- Optionally integrate structural variant calling (Sniffles2) as a new stage.\n",
    "- Convert this workflow into a DAGMan pipeline for multi‑stage orchestration.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash (conda: base) *",
   "language": "bash",
   "name": "conda-base-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
